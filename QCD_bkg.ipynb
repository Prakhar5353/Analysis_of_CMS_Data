{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee17a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef4ae0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_dEscaleDown</th>\n",
       "      <th>lead_dEscaleUp</th>\n",
       "      <th>lead_dEsigmaDown</th>\n",
       "      <th>lead_dEsigmaUp</th>\n",
       "      <th>lead_eCorr</th>\n",
       "      <th>lead_energyErr</th>\n",
       "      <th>lead_energyRaw</th>\n",
       "      <th>lead_esEffSigmaRR</th>\n",
       "      <th>lead_esEnergyOverRawE</th>\n",
       "      <th>lead_eta</th>\n",
       "      <th>...</th>\n",
       "      <th>fixedGridRhoAll</th>\n",
       "      <th>genWeight</th>\n",
       "      <th>dZ</th>\n",
       "      <th>HTXS_Higgs_pt</th>\n",
       "      <th>HTXS_Higgs_y</th>\n",
       "      <th>HTXS_njets30</th>\n",
       "      <th>HTXS_stage_0</th>\n",
       "      <th>weight_central</th>\n",
       "      <th>weight</th>\n",
       "      <th>sigma_m_over_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>-0.003937</td>\n",
       "      <td>1.010513</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>206.3750</td>\n",
       "      <td>4.278493</td>\n",
       "      <td>0.044309</td>\n",
       "      <td>1.925049</td>\n",
       "      <td>...</td>\n",
       "      <td>19.058441</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>82.039062</td>\n",
       "      <td>1.085205</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1.025077</td>\n",
       "      <td>197.593012</td>\n",
       "      <td>0.012727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.990424</td>\n",
       "      <td>0.695312</td>\n",
       "      <td>67.3125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447571</td>\n",
       "      <td>...</td>\n",
       "      <td>24.889784</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>-8.606628</td>\n",
       "      <td>2.443604</td>\n",
       "      <td>0.849854</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.917619</td>\n",
       "      <td>176.879389</td>\n",
       "      <td>0.018026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.999636</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>77.6875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.537231</td>\n",
       "      <td>...</td>\n",
       "      <td>19.327061</td>\n",
       "      <td>-192.759155</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>14.875977</td>\n",
       "      <td>-0.524536</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.943693</td>\n",
       "      <td>-181.905493</td>\n",
       "      <td>0.007757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>-0.004883</td>\n",
       "      <td>1.003078</td>\n",
       "      <td>4.312500</td>\n",
       "      <td>347.5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.543091</td>\n",
       "      <td>...</td>\n",
       "      <td>16.903391</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>404.562500</td>\n",
       "      <td>-0.408264</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.979979</td>\n",
       "      <td>188.899965</td>\n",
       "      <td>0.008444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>-0.007935</td>\n",
       "      <td>1.003633</td>\n",
       "      <td>10.625000</td>\n",
       "      <td>282.0000</td>\n",
       "      <td>2.852130</td>\n",
       "      <td>0.008132</td>\n",
       "      <td>2.421387</td>\n",
       "      <td>...</td>\n",
       "      <td>13.719858</td>\n",
       "      <td>-192.759155</td>\n",
       "      <td>3.000793</td>\n",
       "      <td>8.405273</td>\n",
       "      <td>1.736572</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.941476</td>\n",
       "      <td>-181.478198</td>\n",
       "      <td>0.019778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453384</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>1.000297</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>320.2500</td>\n",
       "      <td>2.316661</td>\n",
       "      <td>0.013519</td>\n",
       "      <td>-2.453613</td>\n",
       "      <td>...</td>\n",
       "      <td>16.007895</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>-0.000580</td>\n",
       "      <td>6.920166</td>\n",
       "      <td>-2.039062</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.849301</td>\n",
       "      <td>163.710503</td>\n",
       "      <td>0.024412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453385</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>-0.002083</td>\n",
       "      <td>1.008488</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>67.5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.050697</td>\n",
       "      <td>...</td>\n",
       "      <td>21.211321</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>41.371094</td>\n",
       "      <td>-0.199677</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.025274</td>\n",
       "      <td>197.630959</td>\n",
       "      <td>0.008993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453386</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>-0.001114</td>\n",
       "      <td>1.003630</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>66.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.341614</td>\n",
       "      <td>...</td>\n",
       "      <td>22.247320</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>2.511253</td>\n",
       "      <td>48.896484</td>\n",
       "      <td>-0.852051</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.934029</td>\n",
       "      <td>180.042667</td>\n",
       "      <td>0.010242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453387</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>1.005987</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>91.3750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882446</td>\n",
       "      <td>...</td>\n",
       "      <td>30.511745</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>-0.008373</td>\n",
       "      <td>34.089844</td>\n",
       "      <td>0.605957</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1.018678</td>\n",
       "      <td>196.359481</td>\n",
       "      <td>0.022808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453388</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.970387</td>\n",
       "      <td>7.937500</td>\n",
       "      <td>278.7500</td>\n",
       "      <td>5.380006</td>\n",
       "      <td>0.050785</td>\n",
       "      <td>1.686279</td>\n",
       "      <td>...</td>\n",
       "      <td>24.365383</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>-0.001892</td>\n",
       "      <td>99.394531</td>\n",
       "      <td>1.415283</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.994213</td>\n",
       "      <td>191.643690</td>\n",
       "      <td>0.014754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1453389 rows × 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lead_dEscaleDown  lead_dEscaleUp  lead_dEsigmaDown  lead_dEsigmaUp  \\\n",
       "0                     0.0             0.0          0.003937       -0.003937   \n",
       "1                     0.0             0.0         -0.002296        0.002296   \n",
       "2                     0.0             0.0         -0.000107        0.000099   \n",
       "3                     0.0             0.0          0.004883       -0.004883   \n",
       "4                     0.0             0.0          0.007874       -0.007935   \n",
       "...                   ...             ...               ...             ...   \n",
       "1453384               0.0             0.0          0.000763       -0.000732   \n",
       "1453385               0.0             0.0          0.002090       -0.002083   \n",
       "1453386               0.0             0.0          0.001106       -0.001114   \n",
       "1453387               0.0             0.0          0.002487       -0.002502   \n",
       "1453388               0.0             0.0         -0.020142        0.020142   \n",
       "\n",
       "         lead_eCorr  lead_energyErr  lead_energyRaw  lead_esEffSigmaRR  \\\n",
       "0          1.010513        4.875000        206.3750           4.278493   \n",
       "1          0.990424        0.695312         67.3125           0.000000   \n",
       "2          0.999636        0.890625         77.6875           0.000000   \n",
       "3          1.003078        4.312500        347.5000           0.000000   \n",
       "4          1.003633       10.625000        282.0000           2.852130   \n",
       "...             ...             ...             ...                ...   \n",
       "1453384    1.000297       13.000000        320.2500           2.316661   \n",
       "1453385    1.008488        0.773438         67.5000           0.000000   \n",
       "1453386    1.003630        0.882812         66.8750           0.000000   \n",
       "1453387    1.005987        1.437500         91.3750           0.000000   \n",
       "1453388    0.970387        7.937500        278.7500           5.380006   \n",
       "\n",
       "         lead_esEnergyOverRawE  lead_eta  ...  fixedGridRhoAll   genWeight  \\\n",
       "0                     0.044309  1.925049  ...        19.058441  192.759155   \n",
       "1                     0.000000  0.447571  ...        24.889784  192.759155   \n",
       "2                     0.000000 -0.537231  ...        19.327061 -192.759155   \n",
       "3                     0.000000 -0.543091  ...        16.903391  192.759155   \n",
       "4                     0.008132  2.421387  ...        13.719858 -192.759155   \n",
       "...                        ...       ...  ...              ...         ...   \n",
       "1453384               0.013519 -2.453613  ...        16.007895  192.759155   \n",
       "1453385               0.000000 -0.050697  ...        21.211321  192.759155   \n",
       "1453386               0.000000 -0.341614  ...        22.247320  192.759155   \n",
       "1453387               0.000000  0.882446  ...        30.511745  192.759155   \n",
       "1453388               0.050785  1.686279  ...        24.365383  192.759155   \n",
       "\n",
       "               dZ  HTXS_Higgs_pt  HTXS_Higgs_y  HTXS_njets30  HTXS_stage_0  \\\n",
       "0        0.001114      82.039062      1.085205             2            11   \n",
       "1       -8.606628       2.443604      0.849854             0            11   \n",
       "2        0.001587      14.875977     -0.524536             1            11   \n",
       "3        0.000519     404.562500     -0.408264             2            11   \n",
       "4        3.000793       8.405273      1.736572             0            11   \n",
       "...           ...            ...           ...           ...           ...   \n",
       "1453384 -0.000580       6.920166     -2.039062             0            11   \n",
       "1453385  0.000549      41.371094     -0.199677             1            11   \n",
       "1453386  2.511253      48.896484     -0.852051             1            11   \n",
       "1453387 -0.008373      34.089844      0.605957             0            11   \n",
       "1453388 -0.001892      99.394531      1.415283             2            11   \n",
       "\n",
       "         weight_central      weight  sigma_m_over_m  \n",
       "0              1.025077  197.593012        0.012727  \n",
       "1              0.917619  176.879389        0.018026  \n",
       "2              0.943693 -181.905493        0.007757  \n",
       "3              0.979979  188.899965        0.008444  \n",
       "4              0.941476 -181.478198        0.019778  \n",
       "...                 ...         ...             ...  \n",
       "1453384        0.849301  163.710503        0.024412  \n",
       "1453385        1.025274  197.630959        0.008993  \n",
       "1453386        0.934029  180.042667        0.010242  \n",
       "1453387        1.018678  196.359481        0.022808  \n",
       "1453388        0.994213  191.643690        0.014754  \n",
       "\n",
       "[1453389 rows x 160 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qcd = pd.read_parquet('qcd_bkg.parquet',engine = 'pyarrow')\n",
    "df_qcd['class'] = [0]*12465\n",
    "#df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eff9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signal = pd.read_parquet('GGH_125_2017UL.parquet',engine = 'pyarrow')\n",
    "#df_signal\n",
    "df_signal['class'] = [1]*1453389\n",
    "df_signal = df_signal.sample(frac=1)\n",
    "df_signal = df_signal.head(12465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de05ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_dEscaleDown</th>\n",
       "      <th>lead_dEscaleUp</th>\n",
       "      <th>lead_dEsigmaDown</th>\n",
       "      <th>lead_dEsigmaUp</th>\n",
       "      <th>lead_eCorr</th>\n",
       "      <th>lead_energyErr</th>\n",
       "      <th>lead_energyRaw</th>\n",
       "      <th>lead_esEffSigmaRR</th>\n",
       "      <th>lead_esEnergyOverRawE</th>\n",
       "      <th>lead_eta</th>\n",
       "      <th>...</th>\n",
       "      <th>genWeight</th>\n",
       "      <th>dZ</th>\n",
       "      <th>HTXS_Higgs_pt</th>\n",
       "      <th>HTXS_Higgs_y</th>\n",
       "      <th>HTXS_njets30</th>\n",
       "      <th>HTXS_stage_0</th>\n",
       "      <th>weight_central</th>\n",
       "      <th>weight</th>\n",
       "      <th>sigma_m_over_m</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>566122</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001369</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>54.93750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.678711</td>\n",
       "      <td>...</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>4.339111</td>\n",
       "      <td>-0.159485</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.892274</td>\n",
       "      <td>171.994052</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128395</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>1.002954</td>\n",
       "      <td>1.140625</td>\n",
       "      <td>87.75000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400818</td>\n",
       "      <td>...</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>-0.004303</td>\n",
       "      <td>50.437500</td>\n",
       "      <td>0.098083</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.005218</td>\n",
       "      <td>193.764939</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699802</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>-0.002243</td>\n",
       "      <td>1.007519</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>161.12500</td>\n",
       "      <td>5.372659</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>-1.823975</td>\n",
       "      <td>...</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>9.861816</td>\n",
       "      <td>-1.170898</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.916787</td>\n",
       "      <td>176.719176</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003174</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.992204</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>85.18750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.753052</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923489</td>\n",
       "      <td>0.923489</td>\n",
       "      <td>0.013831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376723</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>-0.001495</td>\n",
       "      <td>1.002711</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>122.31250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598999</td>\n",
       "      <td>...</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>-0.000931</td>\n",
       "      <td>76.101562</td>\n",
       "      <td>0.687256</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.152817</td>\n",
       "      <td>222.215963</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001862</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.994021</td>\n",
       "      <td>5.062500</td>\n",
       "      <td>123.43750</td>\n",
       "      <td>3.983726</td>\n",
       "      <td>0.071675</td>\n",
       "      <td>1.870361</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.469269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.798008</td>\n",
       "      <td>0.798008</td>\n",
       "      <td>0.023184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>-0.002472</td>\n",
       "      <td>1.001813</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>165.87500</td>\n",
       "      <td>4.484564</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>-2.036621</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.497818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953585</td>\n",
       "      <td>0.953585</td>\n",
       "      <td>0.012197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006668</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.995064</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>84.18750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.252197</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.668039</td>\n",
       "      <td>0.668039</td>\n",
       "      <td>0.195166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602351</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>1.007461</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>57.03125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716431</td>\n",
       "      <td>...</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>-0.047241</td>\n",
       "      <td>13.232910</td>\n",
       "      <td>-0.111618</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.916983</td>\n",
       "      <td>176.756838</td>\n",
       "      <td>0.010975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386595</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.998503</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>123.31250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.502686</td>\n",
       "      <td>...</td>\n",
       "      <td>192.759155</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>91.328125</td>\n",
       "      <td>-0.639893</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.942669</td>\n",
       "      <td>181.708144</td>\n",
       "      <td>0.030775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24930 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lead_dEscaleDown  lead_dEscaleUp  lead_dEsigmaDown  lead_dEsigmaUp  \\\n",
       "566122               0.0             0.0         -0.001369        0.001366   \n",
       "128395               0.0             0.0          0.001183       -0.001183   \n",
       "699802               0.0             0.0          0.002228       -0.002243   \n",
       "5785                 0.0             0.0         -0.003174        0.003181   \n",
       "376723               0.0             0.0          0.001495       -0.001495   \n",
       "...                  ...             ...               ...             ...   \n",
       "1201                 0.0             0.0         -0.001862        0.001862   \n",
       "795                  0.0             0.0          0.002487       -0.002472   \n",
       "2507                 0.0             0.0         -0.006668        0.006668   \n",
       "602351               0.0             0.0          0.001541       -0.001545   \n",
       "386595               0.0             0.0         -0.000656        0.000664   \n",
       "\n",
       "        lead_eCorr  lead_energyErr  lead_energyRaw  lead_esEffSigmaRR  \\\n",
       "566122    0.994681        0.914062        54.93750           0.000000   \n",
       "128395    1.002954        1.140625        87.75000           0.000000   \n",
       "699802    1.007519        4.187500       161.12500           5.372659   \n",
       "5785      0.992204        2.250000        85.18750           0.000000   \n",
       "376723    1.002711        1.750000       122.31250           0.000000   \n",
       "...            ...             ...             ...                ...   \n",
       "1201      0.994021        5.062500       123.43750           3.983726   \n",
       "795       1.001813        3.937500       165.87500           4.484564   \n",
       "2507      0.995064        3.375000        84.18750           0.000000   \n",
       "602351    1.007461        0.703125        57.03125           0.000000   \n",
       "386595    0.998503        1.250000       123.31250           0.000000   \n",
       "\n",
       "        lead_esEnergyOverRawE  lead_eta  ...   genWeight        dZ  \\\n",
       "566122               0.000000  0.678711  ...  192.759155  0.000061   \n",
       "128395               0.000000  0.400818  ...  192.759155 -0.004303   \n",
       "699802               0.043216 -1.823975  ...  192.759155  0.000092   \n",
       "5785                 0.000000  0.753052  ...    1.000000 -0.000183   \n",
       "376723               0.000000  0.598999  ...  192.759155 -0.000931   \n",
       "...                       ...       ...  ...         ...       ...   \n",
       "1201                 0.071675  1.870361  ...    1.000000 -0.469269   \n",
       "795                  0.076700 -2.036621  ...    1.000000  4.497818   \n",
       "2507                 0.000000  1.252197  ...    1.000000 -0.000149   \n",
       "602351               0.000000  0.716431  ...  192.759155 -0.047241   \n",
       "386595               0.000000 -0.502686  ...  192.759155 -0.000282   \n",
       "\n",
       "        HTXS_Higgs_pt  HTXS_Higgs_y  HTXS_njets30  HTXS_stage_0  \\\n",
       "566122       4.339111     -0.159485             0            11   \n",
       "128395      50.437500      0.098083             1            11   \n",
       "699802       9.861816     -1.170898             0            11   \n",
       "5785         0.000000           NaN             0             0   \n",
       "376723      76.101562      0.687256             1            11   \n",
       "...               ...           ...           ...           ...   \n",
       "1201         0.000000           NaN             0             0   \n",
       "795          0.000000           NaN             0             0   \n",
       "2507         0.000000           NaN             0             0   \n",
       "602351      13.232910     -0.111618             0            11   \n",
       "386595      91.328125     -0.639893             3            11   \n",
       "\n",
       "        weight_central      weight  sigma_m_over_m  class  \n",
       "566122        0.892274  171.994052        0.014726      1  \n",
       "128395        1.005218  193.764939        0.008645      1  \n",
       "699802        0.916787  176.719176        0.016976      1  \n",
       "5785          0.923489    0.923489        0.013831      0  \n",
       "376723        1.152817  222.215963        0.015821      1  \n",
       "...                ...         ...             ...    ...  \n",
       "1201          0.798008    0.798008        0.023184      0  \n",
       "795           0.953585    0.953585        0.012197      0  \n",
       "2507          0.668039    0.668039        0.195166      0  \n",
       "602351        0.916983  176.756838        0.010975      1  \n",
       "386595        0.942669  181.708144        0.030775      1  \n",
       "\n",
       "[24930 rows x 161 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_qcd,df_signal],axis=0,join = 'outer')\n",
    "df = df.sample(frac = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b856f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subleadptom'] = df['sublead_pt']/df['mass']\n",
    "df['leadptom'] = df['lead_pt']/df['mass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5074d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d35130e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4e2ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import scienceplots\n",
    "plt.style.use(['science', 'notebook'])\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b77f13b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sublead_mvaID</th>\n",
       "      <th>sigmarv</th>\n",
       "      <th>lead_mvaID</th>\n",
       "      <th>sublead_eta</th>\n",
       "      <th>subleadptom</th>\n",
       "      <th>vtxprob</th>\n",
       "      <th>lead_eta</th>\n",
       "      <th>sigmawv</th>\n",
       "      <th>leadptom</th>\n",
       "      <th>CosPhi</th>\n",
       "      <th>PV_chi2</th>\n",
       "      <th>PV_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>566122</th>\n",
       "      <td>0.919918</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>0.964607</td>\n",
       "      <td>-1.034912</td>\n",
       "      <td>0.347595</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.678711</td>\n",
       "      <td>0.014927</td>\n",
       "      <td>0.372543</td>\n",
       "      <td>-0.995675</td>\n",
       "      <td>0.759766</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128395</th>\n",
       "      <td>0.786014</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.931146</td>\n",
       "      <td>-0.468994</td>\n",
       "      <td>0.338646</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.400818</td>\n",
       "      <td>0.008695</td>\n",
       "      <td>0.652384</td>\n",
       "      <td>-0.860501</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>744.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699802</th>\n",
       "      <td>0.783355</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.847880</td>\n",
       "      <td>-0.419006</td>\n",
       "      <td>0.367452</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-1.823975</td>\n",
       "      <td>0.025410</td>\n",
       "      <td>0.431504</td>\n",
       "      <td>-0.991123</td>\n",
       "      <td>0.759766</td>\n",
       "      <td>1324.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>0.749282</td>\n",
       "      <td>0.013831</td>\n",
       "      <td>-0.573591</td>\n",
       "      <td>0.493103</td>\n",
       "      <td>0.354325</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.753052</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.765935</td>\n",
       "      <td>-0.804170</td>\n",
       "      <td>0.837891</td>\n",
       "      <td>5312.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376723</th>\n",
       "      <td>0.357473</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.967611</td>\n",
       "      <td>0.924927</td>\n",
       "      <td>0.315276</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.598999</td>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.843957</td>\n",
       "      <td>-0.818196</td>\n",
       "      <td>0.740234</td>\n",
       "      <td>6368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>-0.318160</td>\n",
       "      <td>0.023184</td>\n",
       "      <td>-0.243625</td>\n",
       "      <td>2.058594</td>\n",
       "      <td>0.441210</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.870361</td>\n",
       "      <td>0.027016</td>\n",
       "      <td>0.582111</td>\n",
       "      <td>-0.927288</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>696.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>-0.589571</td>\n",
       "      <td>0.012197</td>\n",
       "      <td>0.461023</td>\n",
       "      <td>-0.227081</td>\n",
       "      <td>0.329584</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-2.036621</td>\n",
       "      <td>0.022031</td>\n",
       "      <td>0.372380</td>\n",
       "      <td>-0.938696</td>\n",
       "      <td>0.740234</td>\n",
       "      <td>656.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>-0.993471</td>\n",
       "      <td>0.195166</td>\n",
       "      <td>0.632410</td>\n",
       "      <td>0.281128</td>\n",
       "      <td>0.327250</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.252197</td>\n",
       "      <td>0.195849</td>\n",
       "      <td>0.613270</td>\n",
       "      <td>-0.982255</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>2272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602351</th>\n",
       "      <td>-0.578240</td>\n",
       "      <td>0.010975</td>\n",
       "      <td>0.915845</td>\n",
       "      <td>-0.928467</td>\n",
       "      <td>0.366551</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.716431</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.374081</td>\n",
       "      <td>-0.960920</td>\n",
       "      <td>0.751953</td>\n",
       "      <td>157.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386595</th>\n",
       "      <td>0.852081</td>\n",
       "      <td>0.030775</td>\n",
       "      <td>0.990570</td>\n",
       "      <td>-1.001465</td>\n",
       "      <td>0.332622</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.502686</td>\n",
       "      <td>0.035352</td>\n",
       "      <td>0.873474</td>\n",
       "      <td>-0.602424</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>2040.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24930 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sublead_mvaID   sigmarv  lead_mvaID  sublead_eta  subleadptom  \\\n",
       "566122       0.919918  0.014726    0.964607    -1.034912     0.347595   \n",
       "128395       0.786014  0.008645    0.931146    -0.468994     0.338646   \n",
       "699802       0.783355  0.016976    0.847880    -0.419006     0.367452   \n",
       "5785         0.749282  0.013831   -0.573591     0.493103     0.354325   \n",
       "376723       0.357473  0.015821    0.967611     0.924927     0.315276   \n",
       "...               ...       ...         ...          ...          ...   \n",
       "1201        -0.318160  0.023184   -0.243625     2.058594     0.441210   \n",
       "795         -0.589571  0.012197    0.461023    -0.227081     0.329584   \n",
       "2507        -0.993471  0.195166    0.632410     0.281128     0.327250   \n",
       "602351      -0.578240  0.010975    0.915845    -0.928467     0.366551   \n",
       "386595       0.852081  0.030775    0.990570    -1.001465     0.332622   \n",
       "\n",
       "        vtxprob  lead_eta   sigmawv  leadptom    CosPhi   PV_chi2  PV_Score  \n",
       "566122    0.999  0.678711  0.014927  0.372543 -0.995675  0.759766     125.0  \n",
       "128395    0.999  0.400818  0.008695  0.652384 -0.860501  0.656250     744.0  \n",
       "699802    0.999 -1.823975  0.025410  0.431504 -0.991123  0.759766    1324.0  \n",
       "5785      0.999  0.753052  0.021370  0.765935 -0.804170  0.837891    5312.0  \n",
       "376723    0.999  0.598999  0.023617  0.843957 -0.818196  0.740234    6368.0  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "1201      0.999  1.870361  0.027016  0.582111 -0.927288  0.767578     696.0  \n",
       "795       0.999 -2.036621  0.022031  0.372380 -0.938696  0.740234     656.0  \n",
       "2507      0.999  1.252197  0.195849  0.613270 -0.982255  0.648438    2272.0  \n",
       "602351    0.999  0.716431  0.011083  0.374081 -0.960920  0.751953     157.5  \n",
       "386595    0.999 -0.502686  0.035352  0.873474 -0.602424  0.718750    2040.0  \n",
       "\n",
       "[24930 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['class']\n",
    "features = df[['sublead_mvaID','sigmarv','lead_mvaID','sublead_eta','subleadptom','vtxprob','lead_eta','sigmawv','leadptom','CosPhi','PV_chi2','PV_Score']]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "447aca69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sublead_mvaID</th>\n",
       "      <th>sigmarv</th>\n",
       "      <th>lead_mvaID</th>\n",
       "      <th>sublead_eta</th>\n",
       "      <th>subleadptom</th>\n",
       "      <th>vtxprob</th>\n",
       "      <th>lead_eta</th>\n",
       "      <th>sigmawv</th>\n",
       "      <th>leadptom</th>\n",
       "      <th>CosPhi</th>\n",
       "      <th>PV_chi2</th>\n",
       "      <th>PV_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "      <td>2.493000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.356953e-18</td>\n",
       "      <td>-3.383441e-17</td>\n",
       "      <td>3.326661e-17</td>\n",
       "      <td>-4.978858e-18</td>\n",
       "      <td>-3.577497e-16</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-8.256532e-18</td>\n",
       "      <td>4.926977e-17</td>\n",
       "      <td>-1.729686e-17</td>\n",
       "      <td>-1.980521e-16</td>\n",
       "      <td>2.462787e-16</td>\n",
       "      <td>4.614309e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "      <td>1.000020e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.603478e+00</td>\n",
       "      <td>-5.335303e-01</td>\n",
       "      <td>-1.513166e+00</td>\n",
       "      <td>-2.016184e+00</td>\n",
       "      <td>-6.642920e-01</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-2.003774e+00</td>\n",
       "      <td>-6.259059e-01</td>\n",
       "      <td>-6.861515e-01</td>\n",
       "      <td>-6.299419e-01</td>\n",
       "      <td>-5.223583e+00</td>\n",
       "      <td>-2.450687e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.127476e+00</td>\n",
       "      <td>-4.447040e-01</td>\n",
       "      <td>-1.191687e+00</td>\n",
       "      <td>-7.189489e-01</td>\n",
       "      <td>-3.375503e-01</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-7.148135e-01</td>\n",
       "      <td>-4.174431e-01</td>\n",
       "      <td>-3.471261e-01</td>\n",
       "      <td>-6.119749e-01</td>\n",
       "      <td>-4.904521e-01</td>\n",
       "      <td>-2.215233e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.130342e-01</td>\n",
       "      <td>-3.485139e-01</td>\n",
       "      <td>5.274870e-01</td>\n",
       "      <td>7.191484e-03</td>\n",
       "      <td>-1.019937e-01</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>1.417473e-02</td>\n",
       "      <td>-3.244151e-01</td>\n",
       "      <td>-1.837483e-01</td>\n",
       "      <td>-4.961865e-01</td>\n",
       "      <td>-4.313987e-02</td>\n",
       "      <td>-1.964723e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.984108e-01</td>\n",
       "      <td>-1.406248e-01</td>\n",
       "      <td>9.154320e-01</td>\n",
       "      <td>7.063420e-01</td>\n",
       "      <td>1.328213e-01</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>7.029896e-01</td>\n",
       "      <td>-1.515145e-01</td>\n",
       "      <td>6.371412e-02</td>\n",
       "      <td>1.040834e-01</td>\n",
       "      <td>4.321294e-01</td>\n",
       "      <td>-8.147967e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.979940e-01</td>\n",
       "      <td>5.328285e+00</td>\n",
       "      <td>9.910879e-01</td>\n",
       "      <td>2.006942e+00</td>\n",
       "      <td>6.817394e+01</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>1.989683e+00</td>\n",
       "      <td>5.385156e+00</td>\n",
       "      <td>4.589866e+01</td>\n",
       "      <td>3.215718e+00</td>\n",
       "      <td>6.924832e+01</td>\n",
       "      <td>4.048516e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sublead_mvaID       sigmarv    lead_mvaID   sublead_eta   subleadptom  \\\n",
       "count   2.493000e+04  2.493000e+04  2.493000e+04  2.493000e+04  2.493000e+04   \n",
       "mean    7.356953e-18 -3.383441e-17  3.326661e-17 -4.978858e-18 -3.577497e-16   \n",
       "std     1.000020e+00  1.000020e+00  1.000020e+00  1.000020e+00  1.000020e+00   \n",
       "min    -1.603478e+00 -5.335303e-01 -1.513166e+00 -2.016184e+00 -6.642920e-01   \n",
       "25%    -1.127476e+00 -4.447040e-01 -1.191687e+00 -7.189489e-01 -3.375503e-01   \n",
       "50%     5.130342e-01 -3.485139e-01  5.274870e-01  7.191484e-03 -1.019937e-01   \n",
       "75%     8.984108e-01 -1.406248e-01  9.154320e-01  7.063420e-01  1.328213e-01   \n",
       "max     9.979940e-01  5.328285e+00  9.910879e-01  2.006942e+00  6.817394e+01   \n",
       "\n",
       "            vtxprob      lead_eta       sigmawv      leadptom        CosPhi  \\\n",
       "count  2.493000e+04  2.493000e+04  2.493000e+04  2.493000e+04  2.493000e+04   \n",
       "mean  -3.330669e-16 -8.256532e-18  4.926977e-17 -1.729686e-17 -1.980521e-16   \n",
       "std    0.000000e+00  1.000020e+00  1.000020e+00  1.000020e+00  1.000020e+00   \n",
       "min   -3.330669e-16 -2.003774e+00 -6.259059e-01 -6.861515e-01 -6.299419e-01   \n",
       "25%   -3.330669e-16 -7.148135e-01 -4.174431e-01 -3.471261e-01 -6.119749e-01   \n",
       "50%   -3.330669e-16  1.417473e-02 -3.244151e-01 -1.837483e-01 -4.961865e-01   \n",
       "75%   -3.330669e-16  7.029896e-01 -1.515145e-01  6.371412e-02  1.040834e-01   \n",
       "max   -3.330669e-16  1.989683e+00  5.385156e+00  4.589866e+01  3.215718e+00   \n",
       "\n",
       "            PV_chi2      PV_Score  \n",
       "count  2.493000e+04  2.493000e+04  \n",
       "mean   2.462787e-16  4.614309e-17  \n",
       "std    1.000020e+00  1.000020e+00  \n",
       "min   -5.223583e+00 -2.450687e-01  \n",
       "25%   -4.904521e-01 -2.215233e-01  \n",
       "50%   -4.313987e-02 -1.964723e-01  \n",
       "75%    4.321294e-01 -8.147967e-02  \n",
       "max    6.924832e+01  4.048516e+01  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = features.columns\n",
    "df_scaled = StandardScaler().fit(features)\n",
    "dfsc = df_scaled.transform(features)\n",
    "scaled_features = pd.DataFrame(dfsc,\n",
    "                         columns=cols)\n",
    "scaled_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d253f2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24930.00000\n",
       "mean         0.50000\n",
       "std          0.50001\n",
       "min          0.00000\n",
       "25%          0.00000\n",
       "50%          0.50000\n",
       "75%          1.00000\n",
       "max          1.00000\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86f4aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f97476bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8948ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sublead_mvaID</th>\n",
       "      <th>sigmarv</th>\n",
       "      <th>lead_mvaID</th>\n",
       "      <th>sublead_eta</th>\n",
       "      <th>subleadptom</th>\n",
       "      <th>vtxprob</th>\n",
       "      <th>lead_eta</th>\n",
       "      <th>sigmawv</th>\n",
       "      <th>leadptom</th>\n",
       "      <th>CosPhi</th>\n",
       "      <th>PV_chi2</th>\n",
       "      <th>PV_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19944.000000</td>\n",
       "      <td>19944.000000</td>\n",
       "      <td>19944.000000</td>\n",
       "      <td>19944.000000</td>\n",
       "      <td>19944.000000</td>\n",
       "      <td>1.994400e+04</td>\n",
       "      <td>19944.000000</td>\n",
       "      <td>19944.000000</td>\n",
       "      <td>19944.000000</td>\n",
       "      <td>19944.000000</td>\n",
       "      <td>19944.000000</td>\n",
       "      <td>19944.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002754</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>-0.000823</td>\n",
       "      <td>-0.004192</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>-0.001635</td>\n",
       "      <td>-0.003700</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.001836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.999743</td>\n",
       "      <td>0.998840</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>1.004021</td>\n",
       "      <td>0.954277</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.998792</td>\n",
       "      <td>0.949774</td>\n",
       "      <td>0.994417</td>\n",
       "      <td>1.050188</td>\n",
       "      <td>1.012548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.603242</td>\n",
       "      <td>-0.533530</td>\n",
       "      <td>-1.513166</td>\n",
       "      <td>-2.016184</td>\n",
       "      <td>-0.664292</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-2.003774</td>\n",
       "      <td>-0.625906</td>\n",
       "      <td>-0.686151</td>\n",
       "      <td>-0.629942</td>\n",
       "      <td>-5.223583</td>\n",
       "      <td>-0.245069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.123232</td>\n",
       "      <td>-0.445289</td>\n",
       "      <td>-1.181838</td>\n",
       "      <td>-0.722780</td>\n",
       "      <td>-0.337249</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-0.714350</td>\n",
       "      <td>-0.417110</td>\n",
       "      <td>-0.347055</td>\n",
       "      <td>-0.611946</td>\n",
       "      <td>-0.490452</td>\n",
       "      <td>-0.221539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.517957</td>\n",
       "      <td>-0.349924</td>\n",
       "      <td>0.539083</td>\n",
       "      <td>0.006027</td>\n",
       "      <td>-0.100633</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>0.015871</td>\n",
       "      <td>-0.325437</td>\n",
       "      <td>-0.184377</td>\n",
       "      <td>-0.496399</td>\n",
       "      <td>-0.043140</td>\n",
       "      <td>-0.196791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.899416</td>\n",
       "      <td>-0.142104</td>\n",
       "      <td>0.916660</td>\n",
       "      <td>0.710787</td>\n",
       "      <td>0.133379</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>0.704355</td>\n",
       "      <td>-0.152772</td>\n",
       "      <td>0.062180</td>\n",
       "      <td>0.099383</td>\n",
       "      <td>0.432129</td>\n",
       "      <td>-0.080970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.996869</td>\n",
       "      <td>5.325155</td>\n",
       "      <td>0.991088</td>\n",
       "      <td>2.006942</td>\n",
       "      <td>68.173945</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>1.989683</td>\n",
       "      <td>5.380562</td>\n",
       "      <td>45.898655</td>\n",
       "      <td>3.215718</td>\n",
       "      <td>69.248324</td>\n",
       "      <td>36.438240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sublead_mvaID       sigmarv    lead_mvaID   sublead_eta   subleadptom  \\\n",
       "count   19944.000000  19944.000000  19944.000000  19944.000000  19944.000000   \n",
       "mean        0.002754     -0.001778      0.006239     -0.000823     -0.004192   \n",
       "std         0.999743      0.998840      0.999098      1.004021      0.954277   \n",
       "min        -1.603242     -0.533530     -1.513166     -2.016184     -0.664292   \n",
       "25%        -1.123232     -0.445289     -1.181838     -0.722780     -0.337249   \n",
       "50%         0.517957     -0.349924      0.539083      0.006027     -0.100633   \n",
       "75%         0.899416     -0.142104      0.916660      0.710787      0.133379   \n",
       "max         0.996869      5.325155      0.991088      2.006942     68.173945   \n",
       "\n",
       "            vtxprob      lead_eta       sigmawv      leadptom        CosPhi  \\\n",
       "count  1.994400e+04  19944.000000  19944.000000  19944.000000  19944.000000   \n",
       "mean  -3.330669e-16      0.000869     -0.001635     -0.003700     -0.003508   \n",
       "std    0.000000e+00      0.999967      0.998792      0.949774      0.994417   \n",
       "min   -3.330669e-16     -2.003774     -0.625906     -0.686151     -0.629942   \n",
       "25%   -3.330669e-16     -0.714350     -0.417110     -0.347055     -0.611946   \n",
       "50%   -3.330669e-16      0.015871     -0.325437     -0.184377     -0.496399   \n",
       "75%   -3.330669e-16      0.704355     -0.152772      0.062180      0.099383   \n",
       "max   -3.330669e-16      1.989683      5.380562     45.898655      3.215718   \n",
       "\n",
       "            PV_chi2      PV_Score  \n",
       "count  19944.000000  19944.000000  \n",
       "mean       0.003316      0.001836  \n",
       "std        1.050188      1.012548  \n",
       "min       -5.223583     -0.245069  \n",
       "25%       -0.490452     -0.221539  \n",
       "50%       -0.043140     -0.196791  \n",
       "75%        0.432129     -0.080970  \n",
       "max       69.248324     36.438240  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, target,test_size = 0.2, random_state = 0,shuffle = True)\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23c0bba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sublead_mvaID</th>\n",
       "      <th>sigmarv</th>\n",
       "      <th>lead_mvaID</th>\n",
       "      <th>sublead_eta</th>\n",
       "      <th>subleadptom</th>\n",
       "      <th>vtxprob</th>\n",
       "      <th>lead_eta</th>\n",
       "      <th>sigmawv</th>\n",
       "      <th>leadptom</th>\n",
       "      <th>CosPhi</th>\n",
       "      <th>PV_chi2</th>\n",
       "      <th>PV_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18697.000000</td>\n",
       "      <td>18697.000000</td>\n",
       "      <td>18697.000000</td>\n",
       "      <td>18697.000000</td>\n",
       "      <td>18697.000000</td>\n",
       "      <td>1.869700e+04</td>\n",
       "      <td>18697.000000</td>\n",
       "      <td>18697.000000</td>\n",
       "      <td>18697.000000</td>\n",
       "      <td>18697.000000</td>\n",
       "      <td>18697.000000</td>\n",
       "      <td>18697.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>-0.004436</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>-0.004468</td>\n",
       "      <td>-0.005889</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>0.001684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.999528</td>\n",
       "      <td>1.004745</td>\n",
       "      <td>1.000020</td>\n",
       "      <td>1.004967</td>\n",
       "      <td>0.972545</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.997143</td>\n",
       "      <td>1.004812</td>\n",
       "      <td>0.957944</td>\n",
       "      <td>0.990788</td>\n",
       "      <td>1.067654</td>\n",
       "      <td>1.021167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.603242</td>\n",
       "      <td>-0.533530</td>\n",
       "      <td>-1.513166</td>\n",
       "      <td>-2.016184</td>\n",
       "      <td>-0.663981</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-2.003774</td>\n",
       "      <td>-0.625906</td>\n",
       "      <td>-0.686151</td>\n",
       "      <td>-0.629942</td>\n",
       "      <td>-5.223583</td>\n",
       "      <td>-0.245069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.123225</td>\n",
       "      <td>-0.445489</td>\n",
       "      <td>-1.186542</td>\n",
       "      <td>-0.720103</td>\n",
       "      <td>-0.338260</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-0.703404</td>\n",
       "      <td>-0.417283</td>\n",
       "      <td>-0.347176</td>\n",
       "      <td>-0.611849</td>\n",
       "      <td>-0.490452</td>\n",
       "      <td>-0.221587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.512496</td>\n",
       "      <td>-0.349149</td>\n",
       "      <td>0.536882</td>\n",
       "      <td>0.012298</td>\n",
       "      <td>-0.101065</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>0.021441</td>\n",
       "      <td>-0.325308</td>\n",
       "      <td>-0.184603</td>\n",
       "      <td>-0.495395</td>\n",
       "      <td>-0.043140</td>\n",
       "      <td>-0.196727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.899148</td>\n",
       "      <td>-0.140948</td>\n",
       "      <td>0.916979</td>\n",
       "      <td>0.716755</td>\n",
       "      <td>0.132918</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>0.704623</td>\n",
       "      <td>-0.152063</td>\n",
       "      <td>0.062346</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.432129</td>\n",
       "      <td>-0.081990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.996869</td>\n",
       "      <td>5.325155</td>\n",
       "      <td>0.991088</td>\n",
       "      <td>2.006942</td>\n",
       "      <td>68.173945</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>1.989683</td>\n",
       "      <td>5.380562</td>\n",
       "      <td>45.898655</td>\n",
       "      <td>3.215718</td>\n",
       "      <td>69.248324</td>\n",
       "      <td>36.438240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sublead_mvaID       sigmarv    lead_mvaID   sublead_eta   subleadptom  \\\n",
       "count   18697.000000  18697.000000  18697.000000  18697.000000  18697.000000   \n",
       "mean        0.001277      0.001555      0.005254      0.002501     -0.004436   \n",
       "std         0.999528      1.004745      1.000020      1.004967      0.972545   \n",
       "min        -1.603242     -0.533530     -1.513166     -2.016184     -0.663981   \n",
       "25%        -1.123225     -0.445489     -1.186542     -0.720103     -0.338260   \n",
       "50%         0.512496     -0.349149      0.536882      0.012298     -0.101065   \n",
       "75%         0.899148     -0.140948      0.916979      0.716755      0.132918   \n",
       "max         0.996869      5.325155      0.991088      2.006942     68.173945   \n",
       "\n",
       "            vtxprob      lead_eta       sigmawv      leadptom        CosPhi  \\\n",
       "count  1.869700e+04  18697.000000  18697.000000  18697.000000  18697.000000   \n",
       "mean  -3.330669e-16      0.005694      0.001631     -0.004468     -0.005889   \n",
       "std    0.000000e+00      0.997143      1.004812      0.957944      0.990788   \n",
       "min   -3.330669e-16     -2.003774     -0.625906     -0.686151     -0.629942   \n",
       "25%   -3.330669e-16     -0.703404     -0.417283     -0.347176     -0.611849   \n",
       "50%   -3.330669e-16      0.021441     -0.325308     -0.184603     -0.495395   \n",
       "75%   -3.330669e-16      0.704623     -0.152063      0.062346      0.099542   \n",
       "max   -3.330669e-16      1.989683      5.380562     45.898655      3.215718   \n",
       "\n",
       "            PV_chi2      PV_Score  \n",
       "count  18697.000000  18697.000000  \n",
       "mean       0.005561      0.001684  \n",
       "std        1.067654      1.021167  \n",
       "min       -5.223583     -0.245069  \n",
       "25%       -0.490452     -0.221587  \n",
       "50%       -0.043140     -0.196727  \n",
       "75%        0.432129     -0.081990  \n",
       "max       69.248324     36.438240  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.0625, random_state = 42)\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37ac159a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929015    1\n",
       "6233      0\n",
       "404531    1\n",
       "513341    1\n",
       "1771      0\n",
       "         ..\n",
       "751       0\n",
       "1777      0\n",
       "225       0\n",
       "2918      0\n",
       "6160      0\n",
       "Name: class, Length: 18697, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3dea558",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs, training_outputs = torch.tensor(X_train.values, dtype = torch.float).to(device),torch.tensor(y_train.values, dtype = torch.float).reshape(-1, 1).to(device)\n",
    "validation_inputs, validation_outputs = torch.tensor(X_valid.values, dtype = torch.float).to(device),torch.tensor(y_valid.values, dtype = torch.float).reshape(-1, 1).to(device)\n",
    "#training_outputs.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f51fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(training_inputs, training_outputs)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=524, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "813011e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = CustomDataset(validation_inputs, validation_outputs)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=80, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95ea1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self,layer,input_dim):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.hiddenlayer1 = nn.Linear(in_features = self.input_dim, out_features = self.layer[0])\n",
    "        self.activ1 = nn.ReLU()\n",
    "        self.hiddenlayer2 = nn.Linear(in_features = self.layer[0],out_features = self.layer[1])\n",
    "        self.activ2 = nn.ReLU()\n",
    "        self.hiddenlayer3 = nn.Linear(in_features = self.layer[1],out_features = self.layer[2])\n",
    "        self.activ3 = nn.ReLU()\n",
    "        self.hiddenlayer4 = nn.Linear(in_features = self.layer[2],out_features = self.layer[3])\n",
    "        self.activ4 = nn.ReLU()\n",
    "        self.hiddenlayer5 = nn.Linear(in_features = self.layer[3],out_features = self.layer[4])\n",
    "        self.activ5 = nn.Sigmoid()\n",
    "    def forward(self, inputs):\n",
    "        out1 = self.hiddenlayer1(inputs)\n",
    "        out2 = self.activ1(out1)\n",
    "        out3 = self.hiddenlayer2(out2)\n",
    "        out4 = self.activ2(out3)\n",
    "        out5 = self.hiddenlayer3(out4)\n",
    "        out6 = self.activ3(out5)\n",
    "        out7 = self.hiddenlayer4(out6)\n",
    "        out8 = self.activ4(out7)\n",
    "        out9 = self.hiddenlayer5(out8)\n",
    "        output = self.activ5(out9)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61a80656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "Loss = 0.69926,\t val_loss = 0.69773\n",
      "Epoch 2/500\n",
      "Loss = 0.69738,\t val_loss = 0.69589\n",
      "Epoch 3/500\n",
      "Loss = 0.69514,\t val_loss = 0.69353\n",
      "Epoch 4/500\n",
      "Loss = 0.69265,\t val_loss = 0.69097\n",
      "Epoch 5/500\n",
      "Loss = 0.68960,\t val_loss = 0.68836\n",
      "Epoch 6/500\n",
      "Loss = 0.68628,\t val_loss = 0.68369\n",
      "Epoch 7/500\n",
      "Loss = 0.68241,\t val_loss = 0.68070\n",
      "Epoch 8/500\n",
      "Loss = 0.67784,\t val_loss = 0.67590\n",
      "Epoch 9/500\n",
      "Loss = 0.67268,\t val_loss = 0.67072\n",
      "Epoch 10/500\n",
      "Loss = 0.66690,\t val_loss = 0.66464\n",
      "Epoch 11/500\n",
      "Loss = 0.66015,\t val_loss = 0.65791\n",
      "Epoch 12/500\n",
      "Loss = 0.65231,\t val_loss = 0.65044\n",
      "Epoch 13/500\n",
      "Loss = 0.64317,\t val_loss = 0.64094\n",
      "Epoch 14/500\n",
      "Loss = 0.63268,\t val_loss = 0.62994\n",
      "Epoch 15/500\n",
      "Loss = 0.62070,\t val_loss = 0.61822\n",
      "Epoch 16/500\n",
      "Loss = 0.60756,\t val_loss = 0.60527\n",
      "Epoch 17/500\n",
      "Loss = 0.59329,\t val_loss = 0.59193\n",
      "Epoch 18/500\n",
      "Loss = 0.57857,\t val_loss = 0.57758\n",
      "Epoch 19/500\n",
      "Loss = 0.56349,\t val_loss = 0.56413\n",
      "Epoch 20/500\n",
      "Loss = 0.54863,\t val_loss = 0.54959\n",
      "Epoch 21/500\n",
      "Loss = 0.53395,\t val_loss = 0.53590\n",
      "Epoch 22/500\n",
      "Loss = 0.51992,\t val_loss = 0.52287\n",
      "Epoch 23/500\n",
      "Loss = 0.50691,\t val_loss = 0.51020\n",
      "Epoch 24/500\n",
      "Loss = 0.49484,\t val_loss = 0.50008\n",
      "Epoch 25/500\n",
      "Loss = 0.48396,\t val_loss = 0.49057\n",
      "Epoch 26/500\n",
      "Loss = 0.47374,\t val_loss = 0.47958\n",
      "Epoch 27/500\n",
      "Loss = 0.46507,\t val_loss = 0.47221\n",
      "Epoch 28/500\n",
      "Loss = 0.45692,\t val_loss = 0.46618\n",
      "Epoch 29/500\n",
      "Loss = 0.44959,\t val_loss = 0.45620\n",
      "Epoch 30/500\n",
      "Loss = 0.44333,\t val_loss = 0.45299\n",
      "Epoch 31/500\n",
      "Loss = 0.43736,\t val_loss = 0.44764\n",
      "Epoch 32/500\n",
      "Loss = 0.43225,\t val_loss = 0.44168\n",
      "Epoch 33/500\n",
      "Loss = 0.42751,\t val_loss = 0.43984\n",
      "Epoch 34/500\n",
      "Loss = 0.42322,\t val_loss = 0.43445\n",
      "Epoch 35/500\n",
      "Loss = 0.41937,\t val_loss = 0.42916\n",
      "Epoch 36/500\n",
      "Loss = 0.41611,\t val_loss = 0.42814\n",
      "Epoch 37/500\n",
      "Loss = 0.41303,\t val_loss = 0.42795\n",
      "Epoch 38/500\n",
      "Loss = 0.41005,\t val_loss = 0.42182\n",
      "Epoch 39/500\n",
      "Loss = 0.40739,\t val_loss = 0.41978\n",
      "Epoch 40/500\n",
      "Loss = 0.40491,\t val_loss = 0.41851\n",
      "Epoch 41/500\n",
      "Loss = 0.40291,\t val_loss = 0.41316\n",
      "Epoch 42/500\n",
      "Loss = 0.40028,\t val_loss = 0.41463\n",
      "Epoch 43/500\n",
      "Loss = 0.39848,\t val_loss = 0.41278\n",
      "Epoch 44/500\n",
      "Loss = 0.39665,\t val_loss = 0.40841\n",
      "Epoch 45/500\n",
      "Loss = 0.39538,\t val_loss = 0.40745\n",
      "Epoch 46/500\n",
      "Loss = 0.39333,\t val_loss = 0.40753\n",
      "Epoch 47/500\n",
      "Loss = 0.39170,\t val_loss = 0.40640\n",
      "Epoch 48/500\n",
      "Loss = 0.39104,\t val_loss = 0.40532\n",
      "Epoch 49/500\n",
      "Loss = 0.38919,\t val_loss = 0.40472\n",
      "Epoch 50/500\n",
      "Loss = 0.38808,\t val_loss = 0.40141\n",
      "Epoch 51/500\n",
      "Loss = 0.38707,\t val_loss = 0.39771\n",
      "Epoch 52/500\n",
      "Loss = 0.38560,\t val_loss = 0.39767\n",
      "Epoch 53/500\n",
      "Loss = 0.38491,\t val_loss = 0.39792\n",
      "Epoch 54/500\n",
      "Loss = 0.38401,\t val_loss = 0.39973\n",
      "Epoch 55/500\n",
      "Loss = 0.38257,\t val_loss = 0.39648\n",
      "Epoch 56/500\n",
      "Loss = 0.38150,\t val_loss = 0.39446\n",
      "Epoch 57/500\n",
      "Loss = 0.38110,\t val_loss = 0.39518\n",
      "Epoch 58/500\n",
      "Loss = 0.38006,\t val_loss = 0.39441\n",
      "Epoch 59/500\n",
      "Loss = 0.37917,\t val_loss = 0.39480\n",
      "Epoch 60/500\n",
      "Loss = 0.37832,\t val_loss = 0.39321\n",
      "Epoch 61/500\n",
      "Loss = 0.37767,\t val_loss = 0.38934\n",
      "Epoch 62/500\n",
      "Loss = 0.37691,\t val_loss = 0.38952\n",
      "Epoch 63/500\n",
      "Loss = 0.37607,\t val_loss = 0.39045\n",
      "Epoch 64/500\n",
      "Loss = 0.37529,\t val_loss = 0.39083\n",
      "Epoch 65/500\n",
      "Loss = 0.37436,\t val_loss = 0.38828\n",
      "Epoch 66/500\n",
      "Loss = 0.37376,\t val_loss = 0.38652\n",
      "Epoch 67/500\n",
      "Loss = 0.37319,\t val_loss = 0.38870\n",
      "Epoch 68/500\n",
      "Loss = 0.37276,\t val_loss = 0.38596\n",
      "Epoch 69/500\n",
      "Loss = 0.37207,\t val_loss = 0.38573\n",
      "Epoch 70/500\n",
      "Loss = 0.37134,\t val_loss = 0.38315\n",
      "Epoch 71/500\n",
      "Loss = 0.37065,\t val_loss = 0.38632\n",
      "Epoch 72/500\n",
      "Loss = 0.37011,\t val_loss = 0.38344\n",
      "Epoch 73/500\n",
      "Loss = 0.36938,\t val_loss = 0.38091\n",
      "Epoch 74/500\n",
      "Loss = 0.36907,\t val_loss = 0.38601\n",
      "Epoch 75/500\n",
      "Loss = 0.36829,\t val_loss = 0.38077\n",
      "Epoch 76/500\n",
      "Loss = 0.36772,\t val_loss = 0.37992\n",
      "Epoch 77/500\n",
      "Loss = 0.36741,\t val_loss = 0.38162\n",
      "Epoch 78/500\n",
      "Loss = 0.36634,\t val_loss = 0.38099\n",
      "Epoch 79/500\n",
      "Loss = 0.36627,\t val_loss = 0.37918\n",
      "Epoch 80/500\n",
      "Loss = 0.36531,\t val_loss = 0.38004\n",
      "Epoch 81/500\n",
      "Loss = 0.36493,\t val_loss = 0.37995\n",
      "Epoch 82/500\n",
      "Loss = 0.36431,\t val_loss = 0.37657\n",
      "Epoch 83/500\n",
      "Loss = 0.36382,\t val_loss = 0.37767\n",
      "Epoch 84/500\n",
      "Loss = 0.36292,\t val_loss = 0.37720\n",
      "Epoch 85/500\n",
      "Loss = 0.36338,\t val_loss = 0.37815\n",
      "Epoch 86/500\n",
      "Loss = 0.36249,\t val_loss = 0.38028\n",
      "Epoch 87/500\n",
      "Loss = 0.36203,\t val_loss = 0.37758\n",
      "Epoch 88/500\n",
      "Loss = 0.36161,\t val_loss = 0.37518\n",
      "Epoch 89/500\n",
      "Loss = 0.36090,\t val_loss = 0.38042\n",
      "Epoch 90/500\n",
      "Loss = 0.36057,\t val_loss = 0.37622\n",
      "Epoch 91/500\n",
      "Loss = 0.36006,\t val_loss = 0.37616\n",
      "Epoch 92/500\n",
      "Loss = 0.35924,\t val_loss = 0.37421\n",
      "Epoch 93/500\n",
      "Loss = 0.35905,\t val_loss = 0.37224\n",
      "Epoch 94/500\n",
      "Loss = 0.35831,\t val_loss = 0.37397\n",
      "Epoch 95/500\n",
      "Loss = 0.35755,\t val_loss = 0.37077\n",
      "Epoch 96/500\n",
      "Loss = 0.35766,\t val_loss = 0.37211\n",
      "Epoch 97/500\n",
      "Loss = 0.35720,\t val_loss = 0.37015\n",
      "Epoch 98/500\n",
      "Loss = 0.35676,\t val_loss = 0.36983\n",
      "Epoch 99/500\n",
      "Loss = 0.35582,\t val_loss = 0.37252\n",
      "Epoch 100/500\n",
      "Loss = 0.35566,\t val_loss = 0.36902\n",
      "Epoch 101/500\n",
      "Loss = 0.35540,\t val_loss = 0.37019\n",
      "Epoch 102/500\n",
      "Loss = 0.35466,\t val_loss = 0.36803\n",
      "Epoch 103/500\n",
      "Loss = 0.35410,\t val_loss = 0.36858\n",
      "Epoch 104/500\n",
      "Loss = 0.35393,\t val_loss = 0.36885\n",
      "Epoch 105/500\n",
      "Loss = 0.35338,\t val_loss = 0.36874\n",
      "Epoch 106/500\n",
      "Loss = 0.35277,\t val_loss = 0.36648\n",
      "Epoch 107/500\n",
      "Loss = 0.35229,\t val_loss = 0.36740\n",
      "Epoch 108/500\n",
      "Loss = 0.35222,\t val_loss = 0.36810\n",
      "Epoch 109/500\n",
      "Loss = 0.35175,\t val_loss = 0.36516\n",
      "Epoch 110/500\n",
      "Loss = 0.35125,\t val_loss = 0.36469\n",
      "Epoch 111/500\n",
      "Loss = 0.35106,\t val_loss = 0.36842\n",
      "Epoch 112/500\n",
      "Loss = 0.35044,\t val_loss = 0.36527\n",
      "Epoch 113/500\n",
      "Loss = 0.35038,\t val_loss = 0.36580\n",
      "Epoch 114/500\n",
      "Loss = 0.34998,\t val_loss = 0.36269\n",
      "Epoch 115/500\n",
      "Loss = 0.34906,\t val_loss = 0.36174\n",
      "Epoch 116/500\n",
      "Loss = 0.34882,\t val_loss = 0.36371\n",
      "Epoch 117/500\n",
      "Loss = 0.34812,\t val_loss = 0.36649\n",
      "Epoch 118/500\n",
      "Loss = 0.34792,\t val_loss = 0.36180\n",
      "Epoch 119/500\n",
      "Loss = 0.34743,\t val_loss = 0.36313\n",
      "Epoch 120/500\n",
      "Loss = 0.34684,\t val_loss = 0.36177\n",
      "Epoch 121/500\n",
      "Loss = 0.34658,\t val_loss = 0.36182\n",
      "Epoch 122/500\n",
      "Loss = 0.34642,\t val_loss = 0.36290\n",
      "Epoch 123/500\n",
      "Loss = 0.34590,\t val_loss = 0.36190\n",
      "Epoch 124/500\n",
      "Loss = 0.34557,\t val_loss = 0.36154\n",
      "Epoch 125/500\n",
      "Loss = 0.34486,\t val_loss = 0.36001\n",
      "Epoch 126/500\n",
      "Loss = 0.34463,\t val_loss = 0.36096\n",
      "Epoch 127/500\n",
      "Loss = 0.34429,\t val_loss = 0.35799\n",
      "Epoch 128/500\n",
      "Loss = 0.34406,\t val_loss = 0.35859\n",
      "Epoch 129/500\n",
      "Loss = 0.34323,\t val_loss = 0.35657\n",
      "Epoch 130/500\n",
      "Loss = 0.34298,\t val_loss = 0.35650\n",
      "Epoch 131/500\n",
      "Loss = 0.34278,\t val_loss = 0.35589\n",
      "Epoch 132/500\n",
      "Loss = 0.34236,\t val_loss = 0.35700\n",
      "Epoch 133/500\n",
      "Loss = 0.34190,\t val_loss = 0.35923\n",
      "Epoch 134/500\n",
      "Loss = 0.34160,\t val_loss = 0.35747\n",
      "Epoch 135/500\n",
      "Loss = 0.34117,\t val_loss = 0.36511\n",
      "Epoch 136/500\n",
      "Loss = 0.34088,\t val_loss = 0.36186\n",
      "Epoch 137/500\n",
      "Loss = 0.34061,\t val_loss = 0.35764\n",
      "Epoch 138/500\n",
      "Loss = 0.34019,\t val_loss = 0.35434\n",
      "Epoch 139/500\n",
      "Loss = 0.33985,\t val_loss = 0.35542\n",
      "Epoch 140/500\n",
      "Loss = 0.33944,\t val_loss = 0.35376\n",
      "Epoch 141/500\n",
      "Loss = 0.33930,\t val_loss = 0.35415\n",
      "Epoch 142/500\n",
      "Loss = 0.33882,\t val_loss = 0.35629\n",
      "Epoch 143/500\n",
      "Loss = 0.33796,\t val_loss = 0.35128\n",
      "Epoch 144/500\n",
      "Loss = 0.33808,\t val_loss = 0.35072\n",
      "Epoch 145/500\n",
      "Loss = 0.33750,\t val_loss = 0.35326\n",
      "Epoch 146/500\n",
      "Loss = 0.33724,\t val_loss = 0.35133\n",
      "Epoch 147/500\n",
      "Loss = 0.33652,\t val_loss = 0.35121\n",
      "Epoch 148/500\n",
      "Loss = 0.33626,\t val_loss = 0.35331\n",
      "Epoch 149/500\n",
      "Loss = 0.33592,\t val_loss = 0.35370\n",
      "Epoch 150/500\n",
      "Loss = 0.33568,\t val_loss = 0.35137\n",
      "Epoch 151/500\n",
      "Loss = 0.33559,\t val_loss = 0.34899\n",
      "Epoch 152/500\n",
      "Loss = 0.33477,\t val_loss = 0.35310\n",
      "Epoch 153/500\n",
      "Loss = 0.33450,\t val_loss = 0.34843\n",
      "Epoch 154/500\n",
      "Loss = 0.33418,\t val_loss = 0.35098\n",
      "Epoch 155/500\n",
      "Loss = 0.33442,\t val_loss = 0.34921\n",
      "Epoch 156/500\n",
      "Loss = 0.33363,\t val_loss = 0.35025\n",
      "Epoch 157/500\n",
      "Loss = 0.33333,\t val_loss = 0.35084\n",
      "Epoch 158/500\n",
      "Loss = 0.33313,\t val_loss = 0.34856\n",
      "Epoch 159/500\n",
      "Loss = 0.33257,\t val_loss = 0.35267\n",
      "Epoch 160/500\n",
      "Loss = 0.33253,\t val_loss = 0.34500\n",
      "Epoch 161/500\n",
      "Loss = 0.33192,\t val_loss = 0.34837\n",
      "Epoch 162/500\n",
      "Loss = 0.33136,\t val_loss = 0.34650\n",
      "Epoch 163/500\n",
      "Loss = 0.33132,\t val_loss = 0.34766\n",
      "Epoch 164/500\n",
      "Loss = 0.33054,\t val_loss = 0.34773\n",
      "Epoch 165/500\n",
      "Loss = 0.33029,\t val_loss = 0.34540\n",
      "Epoch 166/500\n",
      "Loss = 0.33027,\t val_loss = 0.34566\n",
      "Epoch 167/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.33007,\t val_loss = 0.34462\n",
      "Epoch 168/500\n",
      "Loss = 0.32957,\t val_loss = 0.34912\n",
      "Epoch 169/500\n",
      "Loss = 0.32912,\t val_loss = 0.34325\n",
      "Epoch 170/500\n",
      "Loss = 0.32890,\t val_loss = 0.34393\n",
      "Epoch 171/500\n",
      "Loss = 0.32882,\t val_loss = 0.34173\n",
      "Epoch 172/500\n",
      "Loss = 0.32843,\t val_loss = 0.34454\n",
      "Epoch 173/500\n",
      "Loss = 0.32832,\t val_loss = 0.34319\n",
      "Epoch 174/500\n",
      "Loss = 0.32756,\t val_loss = 0.34159\n",
      "Epoch 175/500\n",
      "Loss = 0.32717,\t val_loss = 0.34416\n",
      "Epoch 176/500\n",
      "Loss = 0.32704,\t val_loss = 0.34626\n",
      "Epoch 177/500\n",
      "Loss = 0.32691,\t val_loss = 0.34186\n",
      "Epoch 178/500\n",
      "Loss = 0.32638,\t val_loss = 0.34028\n",
      "Epoch 179/500\n",
      "Loss = 0.32620,\t val_loss = 0.34005\n",
      "Epoch 180/500\n",
      "Loss = 0.32580,\t val_loss = 0.34074\n",
      "Epoch 181/500\n",
      "Loss = 0.32548,\t val_loss = 0.34049\n",
      "Epoch 182/500\n",
      "Loss = 0.32496,\t val_loss = 0.34094\n",
      "Epoch 183/500\n",
      "Loss = 0.32484,\t val_loss = 0.33786\n",
      "Epoch 184/500\n",
      "Loss = 0.32446,\t val_loss = 0.33818\n",
      "Epoch 185/500\n",
      "Loss = 0.32413,\t val_loss = 0.34346\n",
      "Epoch 186/500\n",
      "Loss = 0.32360,\t val_loss = 0.33915\n",
      "Epoch 187/500\n",
      "Loss = 0.32378,\t val_loss = 0.34005\n",
      "Epoch 188/500\n",
      "Loss = 0.32324,\t val_loss = 0.34047\n",
      "Epoch 189/500\n",
      "Loss = 0.32316,\t val_loss = 0.33830\n",
      "Epoch 190/500\n",
      "Loss = 0.32270,\t val_loss = 0.33820\n",
      "Epoch 191/500\n",
      "Loss = 0.32215,\t val_loss = 0.34159\n",
      "Epoch 192/500\n",
      "Loss = 0.32208,\t val_loss = 0.33725\n",
      "Epoch 193/500\n",
      "Loss = 0.32187,\t val_loss = 0.33759\n",
      "Epoch 194/500\n",
      "Loss = 0.32165,\t val_loss = 0.33758\n",
      "Epoch 195/500\n",
      "Loss = 0.32110,\t val_loss = 0.33550\n",
      "Epoch 196/500\n",
      "Loss = 0.32098,\t val_loss = 0.33607\n",
      "Epoch 197/500\n",
      "Loss = 0.32063,\t val_loss = 0.33788\n",
      "Epoch 198/500\n",
      "Loss = 0.32035,\t val_loss = 0.33843\n",
      "Epoch 199/500\n",
      "Loss = 0.32019,\t val_loss = 0.33483\n",
      "Epoch 200/500\n",
      "Loss = 0.31970,\t val_loss = 0.33527\n",
      "Epoch 201/500\n",
      "Loss = 0.31911,\t val_loss = 0.33724\n",
      "Epoch 202/500\n",
      "Loss = 0.31907,\t val_loss = 0.33771\n",
      "Epoch 203/500\n",
      "Loss = 0.31860,\t val_loss = 0.33388\n",
      "Epoch 204/500\n",
      "Loss = 0.31843,\t val_loss = 0.33420\n",
      "Epoch 205/500\n",
      "Loss = 0.31803,\t val_loss = 0.33218\n",
      "Epoch 206/500\n",
      "Loss = 0.31797,\t val_loss = 0.33122\n",
      "Epoch 207/500\n",
      "Loss = 0.31791,\t val_loss = 0.33116\n",
      "Epoch 208/500\n",
      "Loss = 0.31768,\t val_loss = 0.33267\n",
      "Epoch 209/500\n",
      "Loss = 0.31756,\t val_loss = 0.33269\n",
      "Epoch 210/500\n",
      "Loss = 0.31662,\t val_loss = 0.33174\n",
      "Epoch 211/500\n",
      "Loss = 0.31640,\t val_loss = 0.33233\n",
      "Epoch 212/500\n",
      "Loss = 0.31627,\t val_loss = 0.33215\n",
      "Epoch 213/500\n",
      "Loss = 0.31635,\t val_loss = 0.32953\n",
      "Epoch 214/500\n",
      "Loss = 0.31577,\t val_loss = 0.33489\n",
      "Epoch 215/500\n",
      "Loss = 0.31566,\t val_loss = 0.32960\n",
      "Epoch 216/500\n",
      "Loss = 0.31572,\t val_loss = 0.32907\n",
      "Epoch 217/500\n",
      "Loss = 0.31494,\t val_loss = 0.32924\n",
      "Epoch 218/500\n",
      "Loss = 0.31469,\t val_loss = 0.32800\n",
      "Epoch 219/500\n",
      "Loss = 0.31451,\t val_loss = 0.33005\n",
      "Epoch 220/500\n",
      "Loss = 0.31442,\t val_loss = 0.33038\n",
      "Epoch 221/500\n",
      "Loss = 0.31410,\t val_loss = 0.32823\n",
      "Epoch 222/500\n",
      "Loss = 0.31380,\t val_loss = 0.32677\n",
      "Epoch 223/500\n",
      "Loss = 0.31326,\t val_loss = 0.32706\n",
      "Epoch 224/500\n",
      "Loss = 0.31293,\t val_loss = 0.32752\n",
      "Epoch 225/500\n",
      "Loss = 0.31282,\t val_loss = 0.32800\n",
      "Epoch 226/500\n",
      "Loss = 0.31252,\t val_loss = 0.32820\n",
      "Epoch 227/500\n",
      "Loss = 0.31268,\t val_loss = 0.32726\n",
      "Epoch 228/500\n",
      "Loss = 0.31209,\t val_loss = 0.32469\n",
      "Epoch 229/500\n",
      "Loss = 0.31170,\t val_loss = 0.32532\n",
      "Epoch 230/500\n",
      "Loss = 0.31166,\t val_loss = 0.32757\n",
      "Epoch 231/500\n",
      "Loss = 0.31116,\t val_loss = 0.32533\n",
      "Epoch 232/500\n",
      "Loss = 0.31141,\t val_loss = 0.32398\n",
      "Epoch 233/500\n",
      "Loss = 0.31093,\t val_loss = 0.32490\n",
      "Epoch 234/500\n",
      "Loss = 0.31070,\t val_loss = 0.32455\n",
      "Epoch 235/500\n",
      "Loss = 0.31027,\t val_loss = 0.32795\n",
      "Epoch 236/500\n",
      "Loss = 0.31001,\t val_loss = 0.32474\n",
      "Epoch 237/500\n",
      "Loss = 0.31012,\t val_loss = 0.32758\n",
      "Epoch 238/500\n",
      "Loss = 0.30919,\t val_loss = 0.32896\n",
      "Epoch 239/500\n",
      "Loss = 0.30927,\t val_loss = 0.32684\n",
      "Epoch 240/500\n",
      "Loss = 0.30899,\t val_loss = 0.32302\n",
      "Epoch 241/500\n",
      "Loss = 0.30892,\t val_loss = 0.32097\n",
      "Epoch 242/500\n",
      "Loss = 0.30853,\t val_loss = 0.32116\n",
      "Epoch 243/500\n",
      "Loss = 0.30872,\t val_loss = 0.32436\n",
      "Epoch 244/500\n",
      "Loss = 0.30785,\t val_loss = 0.32168\n",
      "Epoch 245/500\n",
      "Loss = 0.30790,\t val_loss = 0.32146\n",
      "Epoch 246/500\n",
      "Loss = 0.30760,\t val_loss = 0.32265\n",
      "Epoch 247/500\n",
      "Loss = 0.30732,\t val_loss = 0.32248\n",
      "Epoch 248/500\n",
      "Loss = 0.30701,\t val_loss = 0.32317\n",
      "Epoch 249/500\n",
      "Loss = 0.30694,\t val_loss = 0.32192\n",
      "Epoch 250/500\n",
      "Loss = 0.30694,\t val_loss = 0.32008\n",
      "Epoch 251/500\n",
      "Loss = 0.30685,\t val_loss = 0.32091\n",
      "Epoch 252/500\n",
      "Loss = 0.30598,\t val_loss = 0.32142\n",
      "Epoch 253/500\n",
      "Loss = 0.30665,\t val_loss = 0.31994\n",
      "Epoch 254/500\n",
      "Loss = 0.30553,\t val_loss = 0.32073\n",
      "Epoch 255/500\n",
      "Loss = 0.30566,\t val_loss = 0.31802\n",
      "Epoch 256/500\n",
      "Loss = 0.30556,\t val_loss = 0.31804\n",
      "Epoch 257/500\n",
      "Loss = 0.30495,\t val_loss = 0.31936\n",
      "Epoch 258/500\n",
      "Loss = 0.30472,\t val_loss = 0.31986\n",
      "Epoch 259/500\n",
      "Loss = 0.30469,\t val_loss = 0.32022\n",
      "Epoch 260/500\n",
      "Loss = 0.30470,\t val_loss = 0.31904\n",
      "Epoch 261/500\n",
      "Loss = 0.30402,\t val_loss = 0.32141\n",
      "Epoch 262/500\n",
      "Loss = 0.30407,\t val_loss = 0.31784\n",
      "Epoch 263/500\n",
      "Loss = 0.30380,\t val_loss = 0.31712\n",
      "Epoch 264/500\n",
      "Loss = 0.30405,\t val_loss = 0.31901\n",
      "Epoch 265/500\n",
      "Loss = 0.30335,\t val_loss = 0.31617\n",
      "Epoch 266/500\n",
      "Loss = 0.30294,\t val_loss = 0.31738\n",
      "Epoch 267/500\n",
      "Loss = 0.30250,\t val_loss = 0.31624\n",
      "Epoch 268/500\n",
      "Loss = 0.30259,\t val_loss = 0.31525\n",
      "Epoch 269/500\n",
      "Loss = 0.30241,\t val_loss = 0.31886\n",
      "Epoch 270/500\n",
      "Loss = 0.30208,\t val_loss = 0.31724\n",
      "Epoch 271/500\n",
      "Loss = 0.30171,\t val_loss = 0.31761\n",
      "Epoch 272/500\n",
      "Loss = 0.30170,\t val_loss = 0.31608\n",
      "Epoch 273/500\n",
      "Loss = 0.30157,\t val_loss = 0.31590\n",
      "Epoch 274/500\n",
      "Loss = 0.30097,\t val_loss = 0.31452\n",
      "Epoch 275/500\n",
      "Loss = 0.30089,\t val_loss = 0.31596\n",
      "Epoch 276/500\n",
      "Loss = 0.30094,\t val_loss = 0.31483\n",
      "Epoch 277/500\n",
      "Loss = 0.30030,\t val_loss = 0.31415\n",
      "Epoch 278/500\n",
      "Loss = 0.30030,\t val_loss = 0.31274\n",
      "Epoch 279/500\n",
      "Loss = 0.30007,\t val_loss = 0.31651\n",
      "Epoch 280/500\n",
      "Loss = 0.29964,\t val_loss = 0.31549\n",
      "Epoch 281/500\n",
      "Loss = 0.29961,\t val_loss = 0.31440\n",
      "Epoch 282/500\n",
      "Loss = 0.29984,\t val_loss = 0.31139\n",
      "Epoch 283/500\n",
      "Loss = 0.29941,\t val_loss = 0.31267\n",
      "Epoch 284/500\n",
      "Loss = 0.29907,\t val_loss = 0.31508\n",
      "Epoch 285/500\n",
      "Loss = 0.29900,\t val_loss = 0.31270\n",
      "Epoch 286/500\n",
      "Loss = 0.29865,\t val_loss = 0.31123\n",
      "Epoch 287/500\n",
      "Loss = 0.29874,\t val_loss = 0.31401\n",
      "Epoch 288/500\n",
      "Loss = 0.29838,\t val_loss = 0.31324\n",
      "Epoch 289/500\n",
      "Loss = 0.29819,\t val_loss = 0.30994\n",
      "Epoch 290/500\n",
      "Loss = 0.29769,\t val_loss = 0.31267\n",
      "Epoch 291/500\n",
      "Loss = 0.29762,\t val_loss = 0.31258\n",
      "Epoch 292/500\n",
      "Loss = 0.29742,\t val_loss = 0.31282\n",
      "Epoch 293/500\n",
      "Loss = 0.29724,\t val_loss = 0.30907\n",
      "Epoch 294/500\n",
      "Loss = 0.29711,\t val_loss = 0.31407\n",
      "Epoch 295/500\n",
      "Loss = 0.29672,\t val_loss = 0.31189\n",
      "Epoch 296/500\n",
      "Loss = 0.29697,\t val_loss = 0.31082\n",
      "Epoch 297/500\n",
      "Loss = 0.29622,\t val_loss = 0.30959\n",
      "Epoch 298/500\n",
      "Loss = 0.29595,\t val_loss = 0.30918\n",
      "Epoch 299/500\n",
      "Loss = 0.29585,\t val_loss = 0.31357\n",
      "Epoch 300/500\n",
      "Loss = 0.29603,\t val_loss = 0.31170\n",
      "Epoch 301/500\n",
      "Loss = 0.29546,\t val_loss = 0.31303\n",
      "Epoch 302/500\n",
      "Loss = 0.29539,\t val_loss = 0.30931\n",
      "Epoch 303/500\n",
      "Loss = 0.29492,\t val_loss = 0.30737\n",
      "Epoch 304/500\n",
      "Loss = 0.29512,\t val_loss = 0.30828\n",
      "Epoch 305/500\n",
      "Loss = 0.29484,\t val_loss = 0.31526\n",
      "Epoch 306/500\n",
      "Loss = 0.29464,\t val_loss = 0.31179\n",
      "Epoch 307/500\n",
      "Loss = 0.29445,\t val_loss = 0.30787\n",
      "Epoch 308/500\n",
      "Loss = 0.29429,\t val_loss = 0.30847\n",
      "Epoch 309/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m t_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(data[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_layers = [20,10,5,2,1]\n",
    "model = classifier(hidden_layers, 12)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "n_epochs = 500\n",
    "valid_loss = []\n",
    "train_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        t_loss = []\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model.forward(data[0])\n",
    "            #print(prediction)\n",
    "            #print(\"data: \",data[1])\n",
    "            loss = criterion(prediction, data[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t_loss.append(loss)\n",
    "        train_loss.append(torch.tensor(t_loss).mean())\n",
    "        \n",
    "        v_loss = []\n",
    "        for v_step, data in enumerate(validation_dataloader):\n",
    "            with torch.no_grad():\n",
    "                _prediction = model.forward(data[0])\n",
    "                _loss = criterion(_prediction, data[1])\n",
    "                v_loss.append(_loss)\n",
    "        valid_loss.append(torch.tensor(v_loss).mean())\n",
    "\n",
    "        #torch.save(model.state_dict(), f'epoch_{epoch}valloss{valid_loss[epoch]:.3f}.pt')\n",
    "        print(f\"Loss = {train_loss[epoch]:.5f},\\t val_loss = {valid_loss[epoch]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "54a4b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input, test_output = torch.tensor(X_test.values, dtype = torch.float).to(device),torch.tensor(y_test.values, dtype = torch.float).reshape(-1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76a9f7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4986, 1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_output = model.forward(test_input)\n",
    "pred_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0281bc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00678992],\n",
       "       [0.20549524],\n",
       "       [0.79726267],\n",
       "       ...,\n",
       "       [0.79726267],\n",
       "       [0.7383029 ],\n",
       "       [0.01541479]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_output = pred_output.detach().cpu().numpy()\n",
    "test_output = test_output.detach().cpu().numpy()\n",
    "pred_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "23235b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(test_output, pred_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "37c6d037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9354047937507964"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d1ab48a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel(\"false positive rate\")\n",
    "plt.ylabel(\"true positive rate\")\n",
    "plt.grid(True)\n",
    "plt.title(\"ROC for classification with QCD Background\")\n",
    "plt.show()\n",
    "plt.savefig(\"qcd_roc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "85592cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.265974  , 0.2653605 , 0.26460767, 0.2637946 ], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds[(fpr>0.199) & (fpr<0.201)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5695115b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sig = pred_output[test_output ==1]\n",
    "bkg = pred_output[test_output==0]\n",
    "plt.hist(sig,bins = 50,range=(0,1),label = \"Signal\")\n",
    "plt.hist(bkg,bins = 50,range=(0,1),label = \"Background\")\n",
    "plt.title(\"Signal-Background separation in case of QCD Background\")\n",
    "plt.grid(True)\n",
    "plt.legend(frameon=True)\n",
    "plt.show()\n",
    "plt.savefig(\"qcd_sep.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "81506c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8782591255515443"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "mask = pred_output > 0.26\n",
    "pred_output[mask] = 1\n",
    "pred_output[~mask] = 0\n",
    "accuracy_score(test_output,pred_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21fc552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
